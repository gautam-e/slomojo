[
  {
    "objectID": "backprop.html",
    "href": "backprop.html",
    "title": "ðŸ”¥ Forward and backward pass",
    "section": "",
    "text": "Let us put in the code fom the previous notebook, to do the imports an to load the data.\n# Imports\nfrom PythonInterface import Python\n\nlet pathlib = Python.import_module(\"pathlib\") # Python standard library\nlet gzip = Python.import_module(\"gzip\") # Python standard library\nlet pickle = Python.import_module(\"pickle\") # Python standard library\nlet np = Python.import_module(\"numpy\")\n\n# Get the data\npath_gz = pathlib.Path('./lost+found/data/mnist.pkl.gz')\nf = gzip.open(path_gz, 'rb')\nu = pickle._Unpickler(f)\nu.encoding = 'latin1'\ndata = u.load()\n\ndata_train = data[0]\ndata_valid = data[1]\n\nx_train = data_train[0]\ny_train = data_train[1]\ny_train = np.expand_dims(y_train, 1)\n\nx_valid = data_valid[0]\ny_valid = data_valid[1]\ny_valid = np.expand_dims(y_valid, 1)\nf.close()\nfrom DType import DType\nfrom Memory import memset_zero\nfrom Object import object, Attr\nfrom Pointer import DTypePointer, Pointer\nfrom Random import rand\nfrom Range import range\nfrom TargetInfo import dtype_sizeof\n\nstruct Matrix[type: DType]:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        rand(self.data, rows*cols)\n        self.rows = rows\n        self.cols = cols\n\n    fn __copyinit__(inout self, other: Self):\n        self.data = other.data\n        self.rows = other.rows\n        self.cols = other.cols\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn zero(inout self):\n        memset_zero(self.data, self.rows * self.cols)\n\n    @always_inline\n    fn __getitem__(self, y: Int, x: Int) -&gt; SIMD[type, 1]:\n        return self.load[1](y, x)\n\n    @always_inline\n    fn load[nelts:Int](self, y: Int, x: Int) -&gt; SIMD[type, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    @always_inline\n    fn __setitem__(self, y: Int, x: Int, val: SIMD[type, 1]):\n        return self.store[1](y, x, val)\n\n    @always_inline\n    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n        self.data.simd_store[nelts](y * self.cols + x, val)\nfn matrix_dataloader[type: DType]( a:PythonObject, o: Matrix[type], bs: Int) raises -&gt; Matrix[type]:\n    for i in range(bs):\n        for j in range(o.cols):\n            o[i,j] = a[i][j].to_float64().cast[type]()\n    return o\nlet bs: Int = 5 # batch-size\nlet ni: Int = 28*28\n\nlet xb: Matrix[DType.float32] = Matrix[DType.float32](bs,ni)\nlet yb: Matrix[DType.float32] = Matrix[DType.float32](bs,1)\nxb.zero()\nyb.zero()\n\nxb = matrix_dataloader(x_train, xb, bs)\nyb = matrix_dataloader(y_train, yb, bs)\nlet no: Int = 10\nvar w: Matrix[DType.float32] = Matrix[DType.float32](ni, no) # weights\nvar b: Matrix[DType.float32] = Matrix[DType.float32](no, 1) # bias\nb.zero()\nvar res = Matrix[DType.float32](xb.rows, w.cols) # result \nres.zero()\nfrom TargetInfo import dtype_sizeof, dtype_simd_width\nfrom Functional import vectorize\n\nalias nelts = dtype_simd_width[DType.float32]() # The SIMD vector width.\n\nfn lin_vectorized[type: DType](res: Matrix[type], xb: Matrix[type], w: Matrix[type], b: Matrix[type]) raises -&gt; Matrix[type]:\n    for i in range(xb.rows): # 50000\n        for j in range(xb.cols): # 784\n            @parameter\n            fn dotbias[nelts: Int](k: Int):\n                res.store[nelts](i,k, res.load[nelts](i,k) + xb[i,j] * w.load[nelts](j,k) + b.load[nelts](k,0))\n            vectorize[nelts, dotbias](w.cols)\n    return res\nres.zero()\nres = lin_vectorized(res, xb, w, b)\nprint(res.rows)\nprint(res.cols)\n\n5\n10"
  },
  {
    "objectID": "backprop.html#foundations",
    "href": "backprop.html#foundations",
    "title": "ðŸ”¥ Forward and backward pass",
    "section": "Foundations",
    "text": "Foundations\n\nRELu from foundations\n\nfrom Math import max\n\nfn relu_layer[type: DType](acts: Matrix[type], out: Matrix[type]) -&gt; Matrix[type]:\n    @parameter\n    fn relu[nelts: Int](k: Int):\n        #let l = SIMD[type, nelts](0)\n        out.store[nelts](k,0, max[type, nelts](acts.load[nelts](k,0), 0.0))\n    vectorize[nelts, relu](acts.rows)\n    return out\n\nLetâ€™s test that with an example. Firstly a little print function for convenience.\n\nfn print_matrix[type: DType](mat: Matrix[type]):\n    for row in range(mat.rows):\n        for col in range(mat.cols):\n            print(mat[row, col])\n\nAnd now, some dummy data.\n\n# These are like the activations\nvar x = Matrix[DType.float32](4, 1) \nx[3,0] = -0.333 # Intentionally have a negative value here\nprint_matrix[DType.float32](x)\n\n0.76618862152099609\n0.21974173188209534\n0.11567939817905426\n-0.33300000429153442\n\n\n\nvar res = Matrix[DType.float32](x.rows, x.cols) # Matrix to hold the result \nres.zero()\nprint_matrix[DType.float32](res)\n\n0.0\n0.0\n0.0\n0.0\n\n\nSo, the result has been initialized / zeroed out nicely. Letâ€™s see what it shows after calling the RELu function.\n\nvar o = relu_layer[DType.float32](x, res)\nprint_matrix(o)\n\n0.76618862152099609\n0.21974173188209534\n0.11567939817905426\n0.0\n\n\nLooks good. The negative values are being zeroed-out, while the positive values are the same as the input matrix.\nNext up: The loss function.\n\n\nLoss function from the foundations: MSE\n\nfrom Math import pow\n\nfn loss[type: DType](y_pred: Matrix[type], y_true: Matrix[type], res: Matrix[type]):\n    @parameter\n    fn mse[nelts: Int](k: Int):\n        let sum_of_squares = pow[type, nelts](y_pred.load[nelts](k,0) - y_true.load[nelts](k,0), 2).reduce_add()\n        res.store[1](0, 0, res.load[1](0, 0) + sum_of_squares[0])\n    vectorize[nelts, mse](y_pred.rows)\n    res.store[1](0, 0, res.load[1](0, 0) / y_pred.rows )\n\nLet us test that with a couple of examples.\nFirst, if both inputs are zeros, then the loss should be zero.\n\nvar y1 = Matrix[DType.float32](8, 1) \ny1.zero()\nvar y2 = Matrix[DType.float32](8, 1) \ny2.zero()\nvar l = Matrix[DType.float32](1, 1) \nl.zero()\nprint(l[0,0])\n\n0.0\n\n\n\nloss[DType.float32](y1, y2, l)\nprint(l[0,0])\n\n0.0\n\n\nThat looks fine.\nNow if we have two random matrices, we would expect the loss to be non-zero.\n\nvar y1 = Matrix[DType.float32](8, 1) \nvar y2 = Matrix[DType.float32](8, 1) \nvar l = Matrix[DType.float32](1, 1) \nl.zero()\nprint(l[0,0])\n\n0.0\n\n\n\nloss[DType.float32](y1, y2, l)\nprint(l[0,0])\n\n0.21140147745609283\n\n\nAnd if both the vectors are equal, we should get zero loss again.\n\nl.zero()\nloss[DType.float32](y1, y1, l)\nprint(l[0,0])\n\n0.0\n\n\n\n\nGradients and backward pass"
  },
  {
    "objectID": "iter.html",
    "href": "iter.html",
    "title": "ðŸ”¥ Iterator",
    "section": "",
    "text": "Machine learning uses iterators extensively (e.g.Â for Dataloaders). So lets explore how iterators work in Mojo and build one ourself.\nFirstly letâ€™s look at how to implement a simple iterator in Python and then use that as a starting point for a Mojo implementation.\n\nclass Counter:\n    def __init__(self, low, high):\n        self.current = low - 1\n        self.high = high\n\n    def __iter__(self):\n        return self\n\n    def __next__(self): # Python 2: def next(self)\n        self.current += 1\n        if self.current &lt; self.high:\n            return self.current\n        raise StopIteration\n\n\nfor c in Counter(3, 9):\n    print(c)\n\n3\n4\n5\n6\n7\n8\n\n\nNow in Mojo it is similar to Python in that we define a __iter__ and a __next__. However, Mojo doesnâ€™t understand the StopIteration exception (that the __next__ in the Python implementation returns)! So how do we tell Mojo that the Iterator has reached the end? As per the Mojo Changelog, the control flow exits automatically when the length is zero.\nSo, we need to implement a __len__ and make sure it decrements with every call to __next__.\n\n@value\nstruct Counter:\n    var current: Int\n    var len: Int\n\n    fn __init__(inout self, low: Int, high: Int):\n        self.current = low - 1\n        self.len = high - low\n\n    fn __len__(self) -&gt; Int:\n        return self.len\n\n    fn __iter__(self) -&gt; Self:\n        return self\n\n    fn __next__(inout self) -&gt; Int: \n        self.len = self.len - 1\n        self.current = self.current + 1\n        return self.current\n    \nfor c in Counter(3, 9):\n    print(c)\n\n3\n4\n5\n6\n7\n8\n\n\nNote that the @value decorator is used to avoid the boilerplate for the __copyinit__ and __moveinit__ methods. See mojodojo.dev for a short tutorial on the @value decorator."
  },
  {
    "objectID": "data.html",
    "href": "data.html",
    "title": "ðŸ”¥ Dataloader",
    "section": "",
    "text": "from PythonInterface import Python\n\nlet pathlib = Python.import_module(\"pathlib\") # Python standard library\nlet gzip = Python.import_module(\"gzip\") # Python standard library\nlet pickle = Python.import_module(\"pickle\") # Python standard library\nlet np = Python.import_module(\"numpy\")\nWe will work with the MNIST dataset. The goal ist to read and load the data as a Matrix struct."
  },
  {
    "objectID": "data.html#get-the-data",
    "href": "data.html#get-the-data",
    "title": "ðŸ”¥ Dataloader",
    "section": "Get the data",
    "text": "Get the data\n\npath_gz = pathlib.Path('./lost+found/data/mnist.pkl.gz')\nf = gzip.open(path_gz, 'rb')\nu = pickle._Unpickler(f)\nu.encoding = 'latin1'\ndata = u.load()\n\ndata_train = data[0]\ndata_valid = data[1]\n\nx_train = data_train[0]\ny_train = data_train[1]\ny_train = np.expand_dims(y_train, 1)\n\nx_valid = data_valid[0]\ny_valid = data_valid[1]\ny_valid = np.expand_dims(y_valid, 1)\nf.close()"
  },
  {
    "objectID": "data.html#look-at-the-data",
    "href": "data.html#look-at-the-data",
    "title": "ðŸ”¥ Dataloader",
    "section": "Look at the data",
    "text": "Look at the data\n\nprint(x_train[0].shape)\n\n(784,)\n\n\n\nlet mpl = Python.import_module(\"matplotlib\")\nimg = np.reshape(x_train[0], (28,28))\nmpl.pyplot.imshow(img, 'gray')\nmpl.pyplot.show()\n\n\n\n\n\n\n\nSo now we have the data as numpy arrays. The next step is to see how we can get it into a Matrix."
  },
  {
    "objectID": "data.html#matrix",
    "href": "data.html#matrix",
    "title": "ðŸ”¥ Dataloader",
    "section": "Matrix",
    "text": "Matrix\nThe implementation of the Matrix struct below is taken from the Mojo documentation.\n\nfrom DType import DType\nfrom Memory import memset_zero\nfrom Object import object, Attr\nfrom Pointer import DTypePointer, Pointer\nfrom Random import rand\nfrom Range import range\nfrom TargetInfo import dtype_sizeof\n\nstruct Matrix[type: DType]:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        rand(self.data, rows*cols)\n        self.rows = rows\n        self.cols = cols\n\n    fn __copyinit__(inout self, other: Self):\n        self.data = other.data\n        self.rows = other.rows\n        self.cols = other.cols\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn zero(inout self):\n        memset_zero(self.data, self.rows * self.cols)\n\n    @always_inline\n    fn __getitem__(self, y: Int, x: Int) -&gt; SIMD[type, 1]:\n        return self.load[1](y, x)\n\n    @always_inline\n    fn load[nelts:Int](self, y: Int, x: Int) -&gt; SIMD[type, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    @always_inline\n    fn __setitem__(self, y: Int, x: Int, val: SIMD[type, 1]):\n        return self.store[1](y, x, val)\n\n    @always_inline\n    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n        self.data.simd_store[nelts](y * self.cols + x, val)"
  },
  {
    "objectID": "data.html#a-mojo-dataloader",
    "href": "data.html#a-mojo-dataloader",
    "title": "ðŸ”¥ Dataloader",
    "section": "A Mojo dataloader",
    "text": "A Mojo dataloader\nWe need a way of getting a batch of samples. Since Mojo doesnt support yield or anything similar we will resort to a more rustic implementation.\n\nlet bs: Int = 8 # batch-size\nlet ni: Int = x_train.shape[1].__index__() #28*28\n\nlet xb: Matrix[DType.float32] = Matrix[DType.float32](bs,ni) # x batch\nlet yb: Matrix[DType.float32] = Matrix[DType.float32](bs,1) # y batch\nxb.zero()\nyb.zero()\n\n\nfn matrix_dataloader[type: DType]( a:PythonObject, o: Matrix[type], bs: Int, bindex: Int) raises:\n    for i in range(bindex*bs, (bindex+1)*bs):\n        for j in range(o.cols):\n            o[i-bindex*bs,j] = a[i][j].to_float64().cast[type]()\n\n\nmatrix_dataloader(x_train, xb, bs, 0)\nmatrix_dataloader(y_train, yb, bs, 0)\n\nLetâ€™s check a few entries to confirm that worked as expected.\n\nprint(yb.load[8](0,0))\n\n[5.0, 0.0, 4.0, 1.0, 9.0, 2.0, 1.0, 3.0]\n\n\n\nfor row in range(0,8,1):\n    print(y_train[row])\n\n[5]\n[0]\n[4]\n[1]\n[9]\n[2]\n[1]\n[3]\n\n\nLooks good. Now we have a way of getting a batch of data. Next, weâ€™ll build a linear layer based on matmul."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My experiments with ðŸ”¥",
    "section": "",
    "text": "This repo is inspired by fastaiâ€™s approach but does everything slowly, in Mojo ðŸ”¥."
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "My experiments with ðŸ”¥",
    "section": "Dependencies",
    "text": "Dependencies\nWe will use only the packages available in the Mojo Playground. Those are:\n\nThe Python standard library\nmatplotlib\nnumpy"
  },
  {
    "objectID": "index.html#content",
    "href": "index.html#content",
    "title": "My experiments with ðŸ”¥",
    "section": "Content",
    "text": "Content\nExperiments with Mojo: 1. Read in the MNIST dataset from numpy\n2. Define a linear layer\n3. Backprop from scratch\n4. Iterators"
  },
  {
    "objectID": "dataset.html",
    "href": "dataset.html",
    "title": "ðŸ”¥ Dataset struct",
    "section": "",
    "text": "from PythonInterface import Python\n\nlet pathlib = Python.import_module(\"pathlib\") # Python standard library\nlet gzip = Python.import_module(\"gzip\") # Python standard library\nlet pickle = Python.import_module(\"pickle\") # Python standard library\nlet np = Python.import_module(\"numpy\")\n\n\npath_gz = pathlib.Path('./lost+found/data/mnist.pkl.gz')\nf = gzip.open(path_gz, 'rb')\nu = pickle._Unpickler(f)\nu.encoding = 'latin1'\ndata = u.load()\n\ndata_train = data[0]\ndata_valid = data[1]\n\nx_train = data_train[0]\ny_train = data_train[1]\ny_train = np.expand_dims(y_train, 1)\n\nx_valid = data_valid[0]\ny_valid = data_valid[1]\ny_valid = np.expand_dims(y_valid, 1)\nf.close()\n\n\nfrom DType import DType\nfrom Memory import memset_zero\nfrom Object import object, Attr\nfrom Pointer import DTypePointer, Pointer\nfrom Random import rand\nfrom Range import range\nfrom TargetInfo import dtype_sizeof\n\nstruct Matrix[type: DType]:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        rand(self.data, rows*cols)\n        self.rows = rows\n        self.cols = cols\n\n    fn __copyinit__(inout self, other: Self):\n        self.data = other.data\n        self.rows = other.rows\n        self.cols = other.cols\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn zero(inout self):\n        memset_zero(self.data, self.rows * self.cols)\n\n    @always_inline\n    fn __getitem__(self, y: Int, x: Int) -&gt; SIMD[type, 1]:\n        return self.load[1](y, x)\n\n    @always_inline\n    fn load[nelts:Int](self, y: Int, x: Int) -&gt; SIMD[type, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    @always_inline\n    fn __setitem__(self, y: Int, x: Int, val: SIMD[type, 1]):\n        return self.store[1](y, x, val)\n\n    @always_inline\n    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n        self.data.simd_store[nelts](y * self.cols + x, val)\n\n\n@value\nstruct Dataset[type: DType, n_feats: Int]:\n    var np_x: PythonObject\n    var np_y: PythonObject\n    var x: Matrix[type] \n    var y: Matrix[type]\n    var len: Int\n\n    fn __init__(inout self, np_x:PythonObject, np_y:PythonObject) raises:\n        self.np_x = np_x\n        self.np_y = np_y\n        self.x = Matrix[type](1,n_feats)\n        self.x.zero()\n        self.y = Matrix[type](1,1)\n        self.y.zero()\n        self.len = np_x.shape[0].__index__()\n\n    fn __len__(self) -&gt; Int:\n        return self.len\n\n    fn __getitem__(self, i: Int) raises -&gt; Tuple[Matrix[type], Matrix[type]]:\n        self.y[0,0] = self.np_y[i][0].to_float64().cast[type]()\n        for j in range(n_feats):\n            self.x[i,j] = self.np_x[i][j].to_float64().cast[type]()\n        return Tuple(self.x, self.y)\n\n\nvar ds = Dataset[DType.float32, 28*28](x_train, y_train)\nprint(ds.__len__())\nvar ds_item = ds[5]\nprint(ds_item.__len__())\n\n50000\n2\n\n\nLetâ€™s take a stab at creating a Dataloader.\n\n@value\nstruct Dataloader[type: DType, n_feats: Int]:\n    var len: Int\n    var bs: Int\n    var ds: Dataset[type, n_feats]\n    var current: Int\n    var xb: Matrix[type] \n    var yb: Matrix[type]\n\n    fn __init__(inout self, ds: Dataset[type, n_feats], bs: Int):\n        self.ds = ds\n        self.bs = bs\n        self.len = ds.__len__()//bs\n        self.current = 0\n        self.xb = Matrix[type](bs,n_feats)\n        self.xb.zero()\n        self.yb = Matrix[type](bs,1)\n        self.yb.zero()\n\n    fn __len__(self) -&gt; Int:\n        return self.len\n\n    fn __iter__(self) -&gt; Self:\n        return self\n\n    fn __next__(inout self) raises -&gt; Matrix[type]:\n        self.len = self.len - 1\n        for i in range(self.current*self.bs, (self.current+1)*self.bs):\n            for j in range(n_feats):\n                self.xb[i,j] = self.ds[i].get[0, Matrix]()[0,j]\n        self.current = self.current + 1\n        return self.xb\n\nerror: Expression [7]:49:57: invalid call to 'get': result cannot bind generic !mlirtype to memory-only type 'Matrix'\n                self.xb[i,j] = self.ds[i].get[0, Matrix]()[0,j]\n                               ~~~~~~~~~~~~~~~~~~~~~~~~~^~\n\n/.modular/Kernels/mojo/Builtin/Tuple.mojo:58:5: function declared here\n    fn get[i: Int, T: AnyType](self) -&gt; T:\n    ^\n\nexpression failed to parse (no further compiler diagnostics)"
  },
  {
    "objectID": "linear.html",
    "href": "linear.html",
    "title": "ðŸ”¥ Matmul -> Linear Layer",
    "section": "",
    "text": "Let us put in the code fom the previous notebook, to do the imports an to load the data.\n# Imports\nfrom PythonInterface import Python\n\nlet pathlib = Python.import_module(\"pathlib\") # Python standard library\nlet gzip = Python.import_module(\"gzip\") # Python standard library\nlet pickle = Python.import_module(\"pickle\") # Python standard library\nlet np = Python.import_module(\"numpy\")\n\n# Get the data\npath_gz = pathlib.Path('./lost+found/data/mnist.pkl.gz')\nf = gzip.open(path_gz, 'rb')\nu = pickle._Unpickler(f)\nu.encoding = 'latin1'\ndata = u.load()\n\ndata_train = data[0]\ndata_valid = data[1]\n\nx_train = data_train[0]\ny_train = data_train[1]\ny_train = np.expand_dims(y_train, 1)\n\nx_valid = data_valid[0]\ny_valid = data_valid[1]\ny_valid = np.expand_dims(y_valid, 1)\nf.close()\nfrom DType import DType\nfrom Memory import memset_zero\nfrom Object import object, Attr\nfrom Pointer import DTypePointer, Pointer\nfrom Random import rand\nfrom Range import range\nfrom TargetInfo import dtype_sizeof\n\nstruct Matrix[type: DType]:\n    var data: DTypePointer[type]\n    var rows: Int\n    var cols: Int\n\n    fn __init__(inout self, rows: Int, cols: Int):\n        self.data = DTypePointer[type].alloc(rows * cols)\n        rand(self.data, rows*cols)\n        self.rows = rows\n        self.cols = cols\n\n    fn __copyinit__(inout self, other: Self):\n        self.data = other.data\n        self.rows = other.rows\n        self.cols = other.cols\n\n    fn __del__(owned self):\n        self.data.free()\n\n    fn zero(inout self):\n        memset_zero(self.data, self.rows * self.cols)\n\n    @always_inline\n    fn __getitem__(self, y: Int, x: Int) -&gt; SIMD[type, 1]:\n        return self.load[1](y, x)\n\n    @always_inline\n    fn load[nelts:Int](self, y: Int, x: Int) -&gt; SIMD[type, nelts]:\n        return self.data.simd_load[nelts](y * self.cols + x)\n\n    @always_inline\n    fn __setitem__(self, y: Int, x: Int, val: SIMD[type, 1]):\n        return self.store[1](y, x, val)\n\n    @always_inline\n    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n        self.data.simd_store[nelts](y * self.cols + x, val)\nfn matrix_dataloader[type: DType]( a:PythonObject, o: Matrix[type], bs: Int) raises:\n    for i in range(bs):\n        for j in range(o.cols):\n            o[i,j] = a[i][j].to_float64().cast[type]()\nlet bs: Int = 5 # batch-size\nlet ni: Int = x_train.shape[1].to_index() #28*28\n\nlet xb: Matrix[DType.float32] = Matrix[DType.float32](bs,ni)\nlet yb: Matrix[DType.float32] = Matrix[DType.float32](bs,1)\nxb.zero()\nyb.zero()\n\nmatrix_dataloader(x_train, xb, bs)\nmatrix_dataloader(y_train, yb, bs)"
  },
  {
    "objectID": "linear.html#linear-layer-from-foundations",
    "href": "linear.html#linear-layer-from-foundations",
    "title": "ðŸ”¥ Matmul -> Linear Layer",
    "section": "Linear layer from foundations",
    "text": "Linear layer from foundations\nA linear layer is nothing but a matrix multiplication (weights and activations) followed by a vector addition (with the bias term).\nSo the basic idea here is to use the the matmul example functions from the Modular website as a starting point and add the bias term in it.\n\nlet no: Int = 10\nvar w: Matrix[DType.float32] = Matrix[DType.float32](ni, no) # weights\nvar b: Matrix[DType.float32] = Matrix[DType.float32](no, 1) # bias\nb.zero()\nvar res = Matrix[DType.float32](xb.rows, w.cols) # result \nres.zero()\n\n\nfrom TargetInfo import dtype_sizeof, dtype_simd_width\nfrom Functional import vectorize\n\nalias nelts = dtype_simd_width[DType.float32]() # The SIMD vector width.\n\nfn lin_vectorized[type: DType](xb: Matrix[type], w: Matrix[type], b: Matrix[type], res: Matrix[type]) raises:\n    for i in range(xb.rows): # 50000\n        for j in range(xb.cols): # 784\n            @parameter\n            fn dotbias[nelts: Int](k: Int):\n                res.store[nelts](i,k, res.load[nelts](i,k) + xb[i,j] * w.load[nelts](j,k) + b.load[nelts](k,0))\n            vectorize[nelts, dotbias](w.cols)\n\n\nres.zero()\nlin_vectorized(xb, w, b, res)\n\n\nprint(res.rows)\nprint(res.cols)\n\n5\n10"
  }
]