{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "skip_showdoc: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”¥ Forward and backward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put in the code fom the previous notebook, to do the imports an to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PythonInterface import Python\n",
    "\n",
    "let pathlib = Python.import_module(\"pathlib\") # Python standard library\n",
    "let gzip = Python.import_module(\"gzip\") # Python standard library\n",
    "let pickle = Python.import_module(\"pickle\") # Python standard library\n",
    "let np = Python.import_module(\"numpy\")\n",
    "\n",
    "# Get the data\n",
    "path_gz = pathlib.Path('./lost+found/data/mnist.pkl.gz')\n",
    "f = gzip.open(path_gz, 'rb')\n",
    "u = pickle._Unpickler(f)\n",
    "u.encoding = 'latin1'\n",
    "data = u.load()\n",
    "\n",
    "data_train = data[0]\n",
    "data_valid = data[1]\n",
    "\n",
    "x_train = data_train[0]\n",
    "y_train = data_train[1]\n",
    "y_train = np.expand_dims(y_train, 1)\n",
    "\n",
    "x_valid = data_valid[0]\n",
    "y_valid = data_valid[1]\n",
    "y_valid = np.expand_dims(y_valid, 1)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DType import DType\n",
    "from Memory import memset_zero\n",
    "from Object import object, Attr\n",
    "from Pointer import DTypePointer, Pointer\n",
    "from Random import rand\n",
    "from Range import range\n",
    "from TargetInfo import dtype_sizeof\n",
    "\n",
    "struct Matrix[type: DType]:\n",
    "    var data: DTypePointer[type]\n",
    "    var rows: Int\n",
    "    var cols: Int\n",
    "\n",
    "    fn __init__(inout self, rows: Int, cols: Int):\n",
    "        self.data = DTypePointer[type].alloc(rows * cols)\n",
    "        rand(self.data, rows*cols)\n",
    "        self.rows = rows\n",
    "        self.cols = cols\n",
    "\n",
    "    fn __copyinit__(inout self, other: Self):\n",
    "        self.data = other.data\n",
    "        self.rows = other.rows\n",
    "        self.cols = other.cols\n",
    "\n",
    "    fn __del__(owned self):\n",
    "        self.data.free()\n",
    "\n",
    "    fn zero(inout self):\n",
    "        memset_zero(self.data, self.rows * self.cols)\n",
    "\n",
    "    @always_inline\n",
    "    fn __getitem__(self, y: Int, x: Int) -> SIMD[type, 1]:\n",
    "        return self.load[1](y, x)\n",
    "\n",
    "    @always_inline\n",
    "    fn load[nelts:Int](self, y: Int, x: Int) -> SIMD[type, nelts]:\n",
    "        return self.data.simd_load[nelts](y * self.cols + x)\n",
    "\n",
    "    @always_inline\n",
    "    fn __setitem__(self, y: Int, x: Int, val: SIMD[type, 1]):\n",
    "        return self.store[1](y, x, val)\n",
    "\n",
    "    @always_inline\n",
    "    fn store[nelts:Int](self, y: Int, x: Int, val: SIMD[type, nelts]):\n",
    "        self.data.simd_store[nelts](y * self.cols + x, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn matrix_dataloader[type: DType]( a:PythonObject, o: Matrix[type], bs: Int) raises -> Matrix[type]:\n",
    "    for i in range(bs):\n",
    "        for j in range(o.cols):\n",
    "            o[i,j] = a[i][j].to_float64().cast[type]()\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let bs: Int = 5 # batch-size\n",
    "let ni: Int = 28*28\n",
    "\n",
    "let xb: Matrix[DType.float32] = Matrix[DType.float32](bs,ni)\n",
    "let yb: Matrix[DType.float32] = Matrix[DType.float32](bs,1)\n",
    "xb.zero()\n",
    "yb.zero()\n",
    "\n",
    "xb = matrix_dataloader(x_train, xb, bs)\n",
    "yb = matrix_dataloader(y_train, yb, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "let no: Int = 10\n",
    "var w: Matrix[DType.float32] = Matrix[DType.float32](ni, no) # weights\n",
    "var b: Matrix[DType.float32] = Matrix[DType.float32](no, 1) # bias\n",
    "b.zero()\n",
    "var res = Matrix[DType.float32](xb.rows, w.cols) # result \n",
    "res.zero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TargetInfo import dtype_sizeof, dtype_simd_width\n",
    "from Functional import vectorize\n",
    "\n",
    "alias nelts = dtype_simd_width[DType.float32]() # The SIMD vector width.\n",
    "\n",
    "fn lin_vectorized[type: DType](res: Matrix[type], xb: Matrix[type], w: Matrix[type], b: Matrix[type]) raises -> Matrix[type]:\n",
    "    for i in range(xb.rows): # 50000\n",
    "        for j in range(xb.cols): # 784\n",
    "            @parameter\n",
    "            fn dotbias[nelts: Int](k: Int):\n",
    "                res.store[nelts](i,k, res.load[nelts](i,k) + xb[i,j] * w.load[nelts](j,k) + b.load[nelts](k,0))\n",
    "            vectorize[nelts, dotbias](w.cols)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.zero()\n",
    "res = lin_vectorized(res, xb, w, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(res.rows)\n",
    "print(res.cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RELu from foundations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Math import max\n",
    "\n",
    "fn relu_layer[type: DType](acts: Matrix[type], out: Matrix[type]) -> Matrix[type]:\n",
    "    @parameter\n",
    "    fn relu[nelts: Int](k: Int):\n",
    "        #let l = SIMD[type, nelts](0)\n",
    "        out.store[nelts](k,0, max[type, nelts](acts.load[nelts](k,0), 0.0))\n",
    "    vectorize[nelts, relu](acts.rows)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that with an example. Firstly a little print function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn print_matrix[type: DType](mat: Matrix[type]):\n",
    "    for row in range(mat.rows):\n",
    "        for col in range(mat.cols):\n",
    "            print(mat[row, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, some dummy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16424916684627533\n",
      "0.034540385007858276\n",
      "0.25480735301971436\n",
      "-0.33300000429153442\n"
     ]
    }
   ],
   "source": [
    "# These are like the activations\n",
    "var x = Matrix[DType.float32](4, 1) \n",
    "x[3,0] = -0.333 # Intentionally have a negative value here\n",
    "print_matrix[DType.float32](x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "var res = Matrix[DType.float32](x.rows, x.cols) # Matrix to hold the result \n",
    "res.zero()\n",
    "print_matrix[DType.float32](res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the result has been initialized / zeroed out nicely. Let's see what it shows after claling the RELu function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.16424916684627533\n",
      "0.034540385007858276\n",
      "0.25480735301971436\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "var o = relu_layer[DType.float32](x, res)\n",
    "print_matrix(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. The negative values are being zeroed-out, while the positive values are the same as the input matrix.  \n",
    "Next up: The loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function: MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients and backward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
